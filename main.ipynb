pip install Flask Flask_Cors langchain openai pdf2image chromadb pypdf tiktoken


#Process user mesagges and documents using OpenAI 
from langchain import OpenAI

def init_lln():
    global llm, llm_embeddings
    api_key = "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"
    
    '''
    Initiliaze LLM model
    '''
    llm_embeddings = OpenAIEmbeddings(openai_api_key = api_key)
    


from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain

def process_document(document_path):
    global conversation_retrieval_chain, llm, llm_embeddings

    
    document = PyPDFLoader.load(document_path)
    
    #Split document into chunks because  in nlp documents are often split into smaller segments to facilitate text analysis
    splitted_document = CharacterTextSplitter.split_documents(document, chunk_size=1000, chunk_overlap=0)
    print(splitted_document)
    
    
    
    #Create vector store  document chunks because text data is inherently unstructured and difficult for machines to process directly. represent text as numerical vectors. Each document or document chunk is converted into a numerical vector that encodes the semantic information contained in the text.
    #We use llm_embedding  because to perform semantic similarity search on the stored document chunks. 
    db = Chroma.from_documents(splitted_document, llm_embeddings) 
    
    
    # Create a retriever interface from the vector store in order to search and retrieving documents or chunks from the vector store based on their similarity to a given query.
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 2})
    
    
    # Create a conversational retrieval chain from the language model and the retriever
    conversation_retrieval_chain = ConversationalRetrievalChain.from_llm(llm, retriever)
    
    
